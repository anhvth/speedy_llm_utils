{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0081003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfed7e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-13 17:54:43.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_utils.lm.oai_lm\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mUsing default model: openai/Qwen/Qwen3-32B-AWQ\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "lm = OAI_LM(port=8008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d007980",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "response_format must be a pydantic model, <class 'str'> provided",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mOutput\u001b[39;00m(BaseModel):\n\u001b[32m      5\u001b[39m     answer: \u001b[38;5;28mstr\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mlm\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhat is your name?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm_utils/llm_utils/lm/oai_lm.py:148\u001b[39m, in \u001b[36mOAI_LM.__call__\u001b[39m\u001b[34m(self, prompt, messages, response_format, cache, retry_count, port, error, use_loadbalance, must_load_cache, max_tokens, num_retries, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;66;03m# Validate response format if provided\u001b[39;00m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response_format:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# Try to get from cache first\u001b[39;00m\n\u001b[32m    151\u001b[39m cache_id = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm_utils/llm_utils/lm/oai_lm.py:196\u001b[39m, in \u001b[36mOAI_LM._validate_response_format\u001b[39m\u001b[34m(self, response_format)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_response_format\u001b[39m(\u001b[38;5;28mself\u001b[39m, response_format):\n\u001b[32m    195\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate that the response format is a pydantic model.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\n\u001b[32m    197\u001b[39m         response_format, BaseModel\n\u001b[32m    198\u001b[39m     ), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mresponse_format must be a pydantic model, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(response_format)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m provided\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: response_format must be a pydantic model, <class 'str'> provided"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "    answer: str\n",
    "\n",
    "\n",
    "lm(\"what is your name?\", guid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e80924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8008/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "model = models.data[0].id\n",
    "\n",
    "\n",
    "class People(BaseModel):\n",
    "    name: str\n",
    "    age: float\n",
    "    is_student: bool\n",
    "    hobbies: list[str]\n",
    "\n",
    "\n",
    "import re\n",
    "from pydantic import BaseModel\n",
    "from typing import Type\n",
    "\n",
    "\n",
    "import re\n",
    "from pydantic import BaseModel\n",
    "from typing import Type\n",
    "\n",
    "\n",
    "import re\n",
    "from pydantic import BaseModel\n",
    "from typing import Type\n",
    "\n",
    "\n",
    "def guided_regex_generator(\n",
    "    model_class: Type[BaseModel], num_think_token: int = 50\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build a regex that enforces:\n",
    "      - A <think>...</think> block of up to num_think_token*4 chars, THEN directly the JSON\n",
    "        OR\n",
    "      - A <think>...</think> block longer than that, THEN the stop-thinking instruction,\n",
    "        THEN the JSON.\n",
    "\n",
    "    Args:\n",
    "      model_class: your Pydantic model defining the JSON schema.\n",
    "      num_think_token: approximate token budget for <think> (1 token ≈ 4 chars).\n",
    "\n",
    "    Returns:\n",
    "      A single regex pattern string (use with re.DOTALL).\n",
    "    \"\"\"\n",
    "    max_chars = num_think_token * 4\n",
    "\n",
    "    # Build a simple JSON‐field regex from your Pydantic model\n",
    "    parts = []\n",
    "    for name, field in model_class.model_fields.items():\n",
    "        t = field.annotation\n",
    "        if t is str:\n",
    "            val = r'\"[^\"]*\"'\n",
    "        elif t is int:\n",
    "            val = r\"\\d+\"\n",
    "        elif t is float:\n",
    "            val = r\"\\d+(?:\\.\\d+)?\"\n",
    "        elif t is bool:\n",
    "            val = r\"(?:true|false)\"\n",
    "        else:\n",
    "            val = r\".+?\"\n",
    "        parts.append(rf'\"{name}\"\\s*:\\s*{val}')\n",
    "    json_pattern = r\"\\{\\s*\" + r\"\\s*,\\s*\".join(parts) + r\"\\s*\\}\"\n",
    "\n",
    "    # The stop-thinking instruction to inject when over budget\n",
    "\n",
    "    # Branch A: short think, no stop-instruction\n",
    "    short_branch = rf\"<think>\\n\" rf\".{{0,{max_chars}}}\" rf\"</think>\\n\" + json_pattern\n",
    "\n",
    "    # Branch B: long think, require stop-instruction\n",
    "    long_branch = (\n",
    "        rf\"<think>\\n\"\n",
    "        # at least max_chars+1 chars, non-greedy so we don't swallow the stop tag\n",
    "        rf\".{{0,{max_chars}}}\"\n",
    "        r\"\\n...\\nConsidering the limited time by the user, I have to give the solution based on the thinking directly now\\.\\n\"\n",
    "        r\"</think>\\.\\n\\n\" + json_pattern\n",
    "    )\n",
    "\n",
    "    # Combine with alternation, and DOTALL when compiling so . matches newlines\n",
    "    full_regex = rf\"^(?:{short_branch}|{long_branch})$\"\n",
    "    return long_branch#short_branch\n",
    "\n",
    "\n",
    "json_schema = People.model_json_schema()\n",
    "\n",
    "prompt = \"Generate a JSON with the name and age of one random person.\"\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,# + \"\\n\\nAnswer in JSON schema: {}\".format(json_schema),\n",
    "        }\n",
    "\n",
    "    ],\n",
    "    extra_body={\"guided_regex\": guided_regex_generator(People, num_think_token=10)},\n",
    "    max_tokens=100,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02375438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reasoning_content:  None\n",
      "<think>\n",
      "Okay, I need to generate a JSON object that\n",
      "Considering the limited time by the user, I have to give the solution based on the thinking directly now.\n",
      "</think>.\n",
      "\n",
      "{\n",
      "  \"name\": \"Emily Johnson\",\n",
      "  \"age\": 23,\n",
      "  \"is_student\": true,\n",
      "  \"hobbies\": [\"reading\", \"hiking\", \"photography\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# print(\"reasoning_content: \", completion.choices[0].message.reasoning_content)\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819799ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lclzt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
